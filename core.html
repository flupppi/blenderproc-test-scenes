<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Important Papers I have read on the topic">

<title>Literature – blenderproc-test-scenes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-b1527d6dab3c1d2528611d399e709420.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="Literature – blenderproc-test-scenes">
<meta property="og:description" content="Important Papers I have read on the topic">
<meta property="og:site_name" content="blenderproc-test-scenes">
<meta name="twitter:title" content="Literature – blenderproc-test-scenes">
<meta name="twitter:description" content="Important Papers I have read on the topic">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">blenderproc-test-scenes</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./core.html">Literature</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">blenderproc-test-scenes</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./core.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Literature</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./test.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">BlenderProc Test Scenes</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#scenenet-an-annotated-model-generator-for-indoor-scene-understanding" id="toc-scenenet-an-annotated-model-generator-for-indoor-scene-understanding" class="nav-link active" data-scroll-target="#scenenet-an-annotated-model-generator-for-indoor-scene-understanding">SceneNet: an Annotated Model Generator for Indoor Scene Understanding</a></li>
  <li><a href="#scenenet-rgb-d-can-5m-synthetic-images-beat-generic-imagenet-pre-training-on-indoor-segmentation" id="toc-scenenet-rgb-d-can-5m-synthetic-images-beat-generic-imagenet-pre-training-on-indoor-segmentation" class="nav-link" data-scroll-target="#scenenet-rgb-d-can-5m-synthetic-images-beat-generic-imagenet-pre-training-on-indoor-segmentation">SceneNet RGB-D: Can 5M Synthetic Images Beat Generic ImageNet Pre-training on Indoor Segmentation?</a></li>
  <li><a href="#blenderproc-scenenet-main.py" id="toc-blenderproc-scenenet-main.py" class="nav-link" data-scroll-target="#blenderproc-scenenet-main.py">BlenderProc SceneNet main.py</a></li>
  <li><a href="#blenderproc-scenenetloader.py" id="toc-blenderproc-scenenetloader.py" class="nav-link" data-scroll-target="#blenderproc-scenenetloader.py">BlenderProc SceneNetLoader.py</a></li>
  <li><a href="#blenderproc-configuring-the-camera" id="toc-blenderproc-configuring-the-camera" class="nav-link" data-scroll-target="#blenderproc-configuring-the-camera">BlenderProc Configuring the Camera</a></li>
  <li><a href="#the-coco-format" id="toc-the-coco-format" class="nav-link" data-scroll-target="#the-coco-format">The COCO format</a></li>
  <li><a href="#my-takeaways" id="toc-my-takeaways" class="nav-link" data-scroll-target="#my-takeaways">My takeaways</a></li>
  <li><a href="#key-information" id="toc-key-information" class="nav-link" data-scroll-target="#key-information">Key Information</a>
  <ul class="collapse">
  <li><a href="#foo" id="toc-foo" class="nav-link" data-scroll-target="#foo">foo</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/flupppi/blenderproc-test-scenes/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="core.html.md"><i class="bi bi-file-code"></i>CommonMark</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Literature</h1>
</div>

<div>
  <div class="description">
    Important Papers I have read on the topic
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<section id="scenenet-an-annotated-model-generator-for-indoor-scene-understanding" class="level2">
<h2 class="anchored" data-anchor-id="scenenet-an-annotated-model-generator-for-indoor-scene-understanding">SceneNet: an Annotated Model Generator for Indoor Scene Understanding</h2>
<p>SceneNet</p>
<ul>
<li>Framework for generating high-quality annotated 3D scenes</li>
</ul>
<p>Goal:</p>
<ul>
<li>Aid Indoor Scene Understanding</li>
<li>Flexible use for supervised training, 3D reconstruction benchmarks, rendered annotated videos / image sequences.</li>
</ul>
<p>Method:</p>
<ul>
<li>Uses manually-annotated datasets of real-world scenes (e.g.&nbsp;NYUv2)</li>
<li>learn statistics about object co-occurrences, spatial relationships.</li>
<li>Hierarchical simulated annealing optimisation</li>
<li>unlimited number of new annotated scenes</li>
<li>Objects and Textures taken from existing databases</li>
</ul>
<p>Contributions:</p>
<ul>
<li><p>Dataset with 57 scenes, 5 scene categories</p></li>
<li><p>Created by human designers and manually annotated at an object instance level.</p></li>
<li><p>Method to automatically generate new physically realistic scenes.</p></li>
<li><p>Scene generation formulated as an optimisation task</p></li>
<li><p>Object relationships, co-occurence and spatial arrangement learned from base scenes and NYUv2 dataset.</p></li>
<li><p>Introduction of scene variety by sampling objects and textures from libraries.</p></li>
<li><p>What is <a href="https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html">NYUv2</a>?</p>
<ul>
<li>Video Sequences from indoor scenes</li>
<li>Recorded in RGBD using Microsoft Kinect</li>
<li>Partially labeled dense multi-class labels</li>
<li>Indoor Segmentation and Support Inference from RGBD Images</li>
<li>Interpret the major surfaces, objects and support relations of an indoor scene from an RGBD image.</li>
<li>typical, messy, indoor scenes</li>
<li>floor, walls, supporting surfaces, object regions, recover support relationships</li>
<li>how do 3D cues inform a structured 3D interpretation?</li>
<li>Provides dataset of 464 diverse indoor scenes with detailed annotations.</li>
<li>Improved object segmentation by being able to infer support structures</li>
</ul></li>
</ul>
<p>Different Scene Categories: (10 scenes per category, 15-250 objects per scene)</p>
<ul>
<li>Bedrooms</li>
<li>Office Scenes</li>
<li>Kitchens</li>
<li>Living Rooms</li>
<li>Bathrooms</li>
</ul>
<p>Automated Scene Generation</p>
<ul>
<li>Simulated Annealing</li>
<li>Man-made scenes as base (SN-BS, NYUv2)</li>
<li>extract meaningful statistics that allow to generate new configurations of objects</li>
<li>Replace objects in generated scene with objects of same category with are sampled from databases (ModelNet, Archive3D)</li>
<li>Scene generation inspired by automatic furniture placement methods formulating the problem as an energy optimisation problem.</li>
<li>A weighted sum of <strong>constraints</strong> is minimised via simulated annealing:
<ul>
<li>Bounding box intersection: Object bounding boxes should not intersect. Penalise deviation from constraint.</li>
<li>Pairwise distance: Pair together objects that are more likely to co-occur. (Maximum reccomended distance <span class="math inline">\(M\)</span> is a metric in these pairwise constraints)</li>
<li>Visibility: ensure that one object is fully visible from the other (why is this necessary? Probably so that objects are evenly spread around the room and not clumped in one corner, just like normal interior design would be done)</li>
<li>Distance to wall: formulate likeliness of objects to be positioned against the wall in distance</li>
<li>Angle to wall: …and also in angling against the wall</li>
</ul></li>
<li>Plug all these constraints into an overall energy function as a weighted sum of all the partial constraint.</li>
<li>Algorithm proposes configurations and then optimises these values in an annealing process where the pertubations in orientation and position are decreased in each iteration according to the annealing schedule.
<ul>
<li>Initialize with all objects centered at the origin.</li>
<li>Each iteration, variables for randomly selected objects are locally pertubed, until a maximum number of iterations are reached, this accounts as one epoch.</li>
<li>After the epoch check the bounding boxes and visibility constraints, continue next epoch until a feasible configuration is found. (1-3 epochs)</li>
<li>After this Object Placement is finished we get a realisticly cluttered and laid out scene.</li>
</ul></li>
<li>To get even better results object groups are defined and moved together as part of the optimizaion process. This allows each of the grouped objects to be more complex and realistic as if it would be possible with a pure global optimization.</li>
<li>There is no limit to the complexity and combination of the layers of groups in the scene that can be generated.</li>
<li>One problem with the 3d objects is ensuring that they are all the same relative scale, this is done using an approach by Savva et al.</li>
<li>Each object comes untextured and is getting applied a texture from a texture library that is appropriate for it. It is uv-mapped automatically in blender. This doesn’t necessarily realistic textures, also as only whole objects and not subparts of objects are textured individually, but it’s main purpose on providing some visual appearance features is still fulfilled.</li>
</ul>
</section>
<section id="scenenet-rgb-d-can-5m-synthetic-images-beat-generic-imagenet-pre-training-on-indoor-segmentation" class="level2">
<h2 class="anchored" data-anchor-id="scenenet-rgb-d-can-5m-synthetic-images-beat-generic-imagenet-pre-training-on-indoor-segmentation">SceneNet RGB-D: Can 5M Synthetic Images Beat Generic ImageNet Pre-training on Indoor Segmentation?</h2>
<p>SceneNet RGB-D</p>
<ul>
<li>Dataset</li>
<li>Provides pixel-perfect ground truth for scene understanding problems (semantic segmentation, instance segmentation, object detection)</li>
<li>camera poses, depth data (allows optical flow, camera pose estimation, 3d scene labeling)</li>
<li>random scene layouts, physically simulated object configurations.</li>
</ul>
<p>Goal:</p>
<ul>
<li>Comparison of semantic segmentation performance of SceneNet vs.&nbsp;VGG-16 ImageNet.</li>
<li>Both fine tuned on the same SUN RGB-D and NYUv2 Datasets</li>
<li>With Depth data included the performance is even better.</li>
<li>Large-scale synthetic datasets with task-specific labels &gt; real-world generic pre-training</li>
</ul>
<p>What is interesting for me here?</p>
<ul>
<li>There were some open questions in the original SceneNet paper that could be answered here.</li>
<li>More realistic rendering with raytracing</li>
<li>Addition of Physics engine for object placement instead of just the annealing process.</li>
</ul>
<p>The problem of Getting good data</p>
<ul>
<li>A core need in developing automated methods for scene understanding is having good labeled data with as much information as possible.</li>
<li>ImageNet was a first step in this direction.</li>
<li>Obtaining more Data such as RGB-D data is very hard though if done manually.</li>
<li>One step in this direction has been done by sceneNN and scanNet, which use reconstructions from path to get the scene geometry and manually annotated the resulting 3d scenes.</li>
<li>Getting other and more reliable data is even more complicated from the real world, and requires additional equipment or is not even possible, e.g.&nbsp;wen thinking about dynamic scenes.</li>
<li>Generating high quality synthetic data with realistic object placement, rendering, human like camera poses and visual effects has the potential to solve many of these problems effectively</li>
</ul>
<p>Contributions:</p>
<ul>
<li>Very large dataset with high-quality ray-traced RGB-D images, with lighting effects, motion blur, ground truth labels</li>
<li>Dataset Generation pipeline relying on fully automatic randomised methods wherever possible.</li>
<li>Proposition of an algorithm to generate camera trajectories</li>
<li>Comparison of a Pretrained RGB-CNN from synthetic data with one that is trained on real-world data.</li>
</ul>
<p>What is actually interesting here is that they are using this randomized physics based object placement approach and completey left out the object placement approach that we saw in the normal SceneNet</p>
</section>
<section id="blenderproc-scenenet-main.py" class="level2">
<h2 class="anchored" data-anchor-id="blenderproc-scenenet-main.py">BlenderProc SceneNet main.py</h2>
<p>Parser:</p>
<ul>
<li>We read one scene file using the <code>scene_net_obj_path</code>, which also references the associated <code>.mtl</code> material file.</li>
<li>The <code>scene_texture_path</code> defines the folder in which all the textures are stored. These are used to map them to the individual objects corresponding to the object types.</li>
<li>The <code>output_dir</code> is just the path to where the generated hdf5 files will be saved</li>
</ul>
<p>Label Mapping:</p>
<ul>
<li>I don’t really understand how the label mappings work. We use a nyu_idset.csv file, that is a internal file from the blenderproc utilities.</li>
<li>My guess would be theat the load_scenenet method can somehow extract an identifier from the obj or material file, that using this object type mapping can be use to infer object labels in the custom property <code>category_id</code>.</li>
</ul>
<p>Objects:</p>
<ul>
<li>we use the special <code>load_scenenet</code> method that loads the obj file, and maps the textures from the folder to the object using the previously computed label_mapping.</li>
</ul>
<p>Handle Floors and Walls:</p>
<ul>
<li>Look for all walls by filtering the loaded objects by the custom property <code>category_id</code> and looking for the id <code>wall</code>.</li>
<li>From these wall objects we extract floors using a builtin BlenderProc method and rename these newly generated objects as floor.</li>
<li>We do the same with the ceilings, with the same builtin method but now looking for the inverse up-vector.</li>
<li>Both the newly created floors and ceilings get set a custom property of either “floor” and “ceiling”</li>
</ul>
<p>Handle Lighting</p>
<ul>
<li>Lamps should emit light, so we look for lights by their name using a regex and add a light surface with a relatively high emission, and having the emission color of the material defined for the lamp.</li>
<li>Also the ceilings emit a small bit of light for lighting up the whole room, simulating maybe some light coming in from the windows.</li>
</ul>
<p>From the objects we create a bounding volume hierarchy.</p>
<p>Finding a camera location:</p>
<ul>
<li>We try 10000 time to find 5 poses</li>
<li>We sample a location above the floor level, at minimum 1.5 meter and maximum 1.8 meter height.</li>
<li>We check that we don’t stand on a object with the camera</li>
<li>We find some random orientation for the camera</li>
<li>We check that there are no objects directly in front of the camera (1.0 meter)</li>
<li>We check that we have a good coverage of the scene with the objects that we are looking at. For doing that we use the bounding volume hierarchy of the scene.</li>
<li>Once we passed all these checks we add the resulting pose to the valid cam poses and try again until we found 5 of them.</li>
</ul>
<p>Now that we have objects loaded, floors and ceilings seperated, and defined good poses for the camera we render the normal maps, depth map, segmentation maps using the custom property <code>category_id</code></p>
<p>Then we just render the scene and write the results in the hdf5 format to the output dir.</p>
</section>
<section id="blenderproc-scenenetloader.py" class="level2">
<h2 class="anchored" data-anchor-id="blenderproc-scenenetloader.py">BlenderProc SceneNetLoader.py</h2>
</section>
<section id="blenderproc-configuring-the-camera" class="level2">
<h2 class="anchored" data-anchor-id="blenderproc-configuring-the-camera">BlenderProc Configuring the Camera</h2>
<p>The docs talk about how to correctly configure a camera in blenderproc.</p>
<p>There are camera intrinsics and extrinsics that you can define. So focal length, optical center vs.&nbsp;position and orientation in the world.</p>
<p>The intrinsics are represented by a matrix <span class="math inline">\(K = \begin{pmatrix} f_x &amp; 0 &amp; c_x \\ 0 &amp; f_y &amp; c_y \\ 0 &amp; 0 &amp; 1\end{pmatrix}\)</span></p>
<p>Using that matrix and the image width and height we can define the full camera intrinsic parameters.</p>
<p>Alternatively blendproc also can set the camera through blenders own camera parameters, where the focal length or field of view can be defined to set the camera intrinsics.</p>
<p>Now to get a pose for the camera a transformation matrix can be defined that maps from camera to world coordinate system. This is passed onto the camera as a pose. The camera then when rendering unses these poses and renders an image for each of these.</p>
</section>
<section id="the-coco-format" class="level2">
<h2 class="anchored" data-anchor-id="the-coco-format">The COCO format</h2>
<p>JSON format, all annotation share the same basic datastructures.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="dt">"info"</span><span class="fu">:</span> <span class="er">info</span><span class="fu">,</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="dt">"images"</span><span class="fu">:</span> <span class="ot">[</span><span class="er">image</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="dt">"annotations"</span><span class="fu">:</span> <span class="ot">[</span><span class="er">annotation</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="dt">"licenses"</span><span class="fu">:</span> <span class="ot">[</span><span class="er">license</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="my-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="my-takeaways">My takeaways</h2>
<p>Large scale labelled datasets are important for supervised learning algorithms:</p>
<ul>
<li>We dont even want to do supervised learning in any form, but we want labelled datasets of 3d scenes that we can render into rgb-d, semantic segmentation and instance segmentation data.</li>
<li>Instead of loading arbitrary scenes and inferring object semantics, why not just generate arbitrary scenes that are still realistic.</li>
<li>Or at least use pre generated scenes for first simple tasks that already contain all the information that we need.</li>
<li>This full control over a scene and it’s generation, gives us the power to also apply this to scene abstraction.</li>
<li>The concept of there being rules and constrained on how a realistic scene is composed and laid out can be reused when abstracting a scene.</li>
<li>The control over the scene generation allows us to replace individual object instances with abstracted object instances and to create a fully abstracted scene from a composition of abstracted objects.</li>
</ul>
<p>Honestly one bigger question that arises now really is what to do with all that what we know of how BlenderProc works. We have at least some amount of useful tooling now availale. That said here comes then the question of what it really is what we want to achieve at the moment and how to vaidate if we achieved it successfully.</p>
<p>This is kind of an open but an important question that my goal is to define today. I just really dont know what i am doing here today. No clue.</p>
</section>
<section id="key-information" class="level2">
<h2 class="anchored" data-anchor-id="key-information">Key Information</h2>
<ul>
<li>What is abstract?
<ul>
<li>Mathematical Abstraction (bounding with intervals)</li>
<li>Conceptual Abstraction (represent a family of possible renderings rather than one fixed image)</li>
</ul></li>
</ul>
<hr>
<p><a href="https://github.com/flupppi/blenderproc-test-scenes/blob/main/blenderproc_test_scenes/core.py#L9" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="foo" class="level3">
<h3 class="anchored" data-anchor-id="foo">foo</h3>
<blockquote class="blockquote">
<pre><code> foo ()</code></pre>
</blockquote>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/flupppi\.github\.io\/blenderproc-test-scenes");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/flupppi/blenderproc-test-scenes/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>