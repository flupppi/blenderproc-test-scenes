[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Literature",
    "section": "",
    "text": "SceneNet\n\nFramework for generating high-quality annotated 3D scenes\n\nGoal:\n\nAid Indoor Scene Understanding\nFlexible use for supervised training, 3D reconstruction benchmarks, rendered annotated videos / image sequences.\n\nMethod:\n\nUses manually-annotated datasets of real-world scenes (e.g. NYUv2)\nlearn statistics about object co-occurrences, spatial relationships.\nHierarchical simulated annealing optimisation\nunlimited number of new annotated scenes\nObjects and Textures taken from existing databases\n\nContributions:\n\nDataset with 57 scenes, 5 scene categories\nCreated by human designers and manually annotated at an object instance level.\nMethod to automatically generate new physically realistic scenes.\nScene generation formulated as an optimisation task\nObject relationships, co-occurence and spatial arrangement learned from base scenes and NYUv2 dataset.\nIntroduction of scene variety by sampling objects and textures from libraries.\nWhat is NYUv2?\n\nVideo Sequences from indoor scenes\nRecorded in RGBD using Microsoft Kinect\nPartially labeled dense multi-class labels\nIndoor Segmentation and Support Inference from RGBD Images\nInterpret the major surfaces, objects and support relations of an indoor scene from an RGBD image.\ntypical, messy, indoor scenes\nfloor, walls, supporting surfaces, object regions, recover support relationships\nhow do 3D cues inform a structured 3D interpretation?\nProvides dataset of 464 diverse indoor scenes with detailed annotations.\nImproved object segmentation by being able to infer support structures\n\n\nDifferent Scene Categories: (10 scenes per category, 15-250 objects per scene)\n\nBedrooms\nOffice Scenes\nKitchens\nLiving Rooms\nBathrooms\n\nAutomated Scene Generation\n\nSimulated Annealing\nMan-made scenes as base (SN-BS, NYUv2)\nextract meaningful statistics that allow to generate new configurations of objects\nReplace objects in generated scene with objects of same category with are sampled from databases (ModelNet, Archive3D)\nScene generation inspired by automatic furniture placement methods formulating the problem as an energy optimisation problem.\nA weighted sum of constraints is minimised via simulated annealing:\n\nBounding box intersection: Object bounding boxes should not intersect. Penalise deviation from constraint.\nPairwise distance: Pair together objects that are more likely to co-occur. (Maximum reccomended distance \\(M\\) is a metric in these pairwise constraints)\nVisibility: ensure that one object is fully visible from the other (why is this necessary? Probably so that objects are evenly spread around the room and not clumped in one corner, just like normal interior design would be done)\nDistance to wall: formulate likeliness of objects to be positioned against the wall in distance\nAngle to wall: …and also in angling against the wall\n\nPlug all these constraints into an overall energy function as a weighted sum of all the partial constraint.\nAlgorithm proposes configurations and then optimises these values in an annealing process where the pertubations in orientation and position are decreased in each iteration according to the annealing schedule.\n\nInitialize with all objects centered at the origin.\nEach iteration, variables for randomly selected objects are locally pertubed, until a maximum number of iterations are reached, this accounts as one epoch.\nAfter the epoch check the bounding boxes and visibility constraints, continue next epoch until a feasible configuration is found. (1-3 epochs)\nAfter this Object Placement is finished we get a realisticly cluttered and laid out scene.\n\nTo get even better results object groups are defined and moved together as part of the optimizaion process. This allows each of the grouped objects to be more complex and realistic as if it would be possible with a pure global optimization.\nThere is no limit to the complexity and combination of the layers of groups in the scene that can be generated.\nOne problem with the 3d objects is ensuring that they are all the same relative scale, this is done using an approach by Savva et al.\nEach object comes untextured and is getting applied a texture from a texture library that is appropriate for it. It is uv-mapped automatically in blender. This doesn’t necessarily realistic textures, also as only whole objects and not subparts of objects are textured individually, but it’s main purpose on providing some visual appearance features is still fulfilled.",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#scenenet-an-annotated-model-generator-for-indoor-scene-understanding",
    "href": "core.html#scenenet-an-annotated-model-generator-for-indoor-scene-understanding",
    "title": "Literature",
    "section": "",
    "text": "SceneNet\n\nFramework for generating high-quality annotated 3D scenes\n\nGoal:\n\nAid Indoor Scene Understanding\nFlexible use for supervised training, 3D reconstruction benchmarks, rendered annotated videos / image sequences.\n\nMethod:\n\nUses manually-annotated datasets of real-world scenes (e.g. NYUv2)\nlearn statistics about object co-occurrences, spatial relationships.\nHierarchical simulated annealing optimisation\nunlimited number of new annotated scenes\nObjects and Textures taken from existing databases\n\nContributions:\n\nDataset with 57 scenes, 5 scene categories\nCreated by human designers and manually annotated at an object instance level.\nMethod to automatically generate new physically realistic scenes.\nScene generation formulated as an optimisation task\nObject relationships, co-occurence and spatial arrangement learned from base scenes and NYUv2 dataset.\nIntroduction of scene variety by sampling objects and textures from libraries.\nWhat is NYUv2?\n\nVideo Sequences from indoor scenes\nRecorded in RGBD using Microsoft Kinect\nPartially labeled dense multi-class labels\nIndoor Segmentation and Support Inference from RGBD Images\nInterpret the major surfaces, objects and support relations of an indoor scene from an RGBD image.\ntypical, messy, indoor scenes\nfloor, walls, supporting surfaces, object regions, recover support relationships\nhow do 3D cues inform a structured 3D interpretation?\nProvides dataset of 464 diverse indoor scenes with detailed annotations.\nImproved object segmentation by being able to infer support structures\n\n\nDifferent Scene Categories: (10 scenes per category, 15-250 objects per scene)\n\nBedrooms\nOffice Scenes\nKitchens\nLiving Rooms\nBathrooms\n\nAutomated Scene Generation\n\nSimulated Annealing\nMan-made scenes as base (SN-BS, NYUv2)\nextract meaningful statistics that allow to generate new configurations of objects\nReplace objects in generated scene with objects of same category with are sampled from databases (ModelNet, Archive3D)\nScene generation inspired by automatic furniture placement methods formulating the problem as an energy optimisation problem.\nA weighted sum of constraints is minimised via simulated annealing:\n\nBounding box intersection: Object bounding boxes should not intersect. Penalise deviation from constraint.\nPairwise distance: Pair together objects that are more likely to co-occur. (Maximum reccomended distance \\(M\\) is a metric in these pairwise constraints)\nVisibility: ensure that one object is fully visible from the other (why is this necessary? Probably so that objects are evenly spread around the room and not clumped in one corner, just like normal interior design would be done)\nDistance to wall: formulate likeliness of objects to be positioned against the wall in distance\nAngle to wall: …and also in angling against the wall\n\nPlug all these constraints into an overall energy function as a weighted sum of all the partial constraint.\nAlgorithm proposes configurations and then optimises these values in an annealing process where the pertubations in orientation and position are decreased in each iteration according to the annealing schedule.\n\nInitialize with all objects centered at the origin.\nEach iteration, variables for randomly selected objects are locally pertubed, until a maximum number of iterations are reached, this accounts as one epoch.\nAfter the epoch check the bounding boxes and visibility constraints, continue next epoch until a feasible configuration is found. (1-3 epochs)\nAfter this Object Placement is finished we get a realisticly cluttered and laid out scene.\n\nTo get even better results object groups are defined and moved together as part of the optimizaion process. This allows each of the grouped objects to be more complex and realistic as if it would be possible with a pure global optimization.\nThere is no limit to the complexity and combination of the layers of groups in the scene that can be generated.\nOne problem with the 3d objects is ensuring that they are all the same relative scale, this is done using an approach by Savva et al.\nEach object comes untextured and is getting applied a texture from a texture library that is appropriate for it. It is uv-mapped automatically in blender. This doesn’t necessarily realistic textures, also as only whole objects and not subparts of objects are textured individually, but it’s main purpose on providing some visual appearance features is still fulfilled.",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#scenenet-rgb-d-can-5m-synthetic-images-beat-generic-imagenet-pre-training-on-indoor-segmentation",
    "href": "core.html#scenenet-rgb-d-can-5m-synthetic-images-beat-generic-imagenet-pre-training-on-indoor-segmentation",
    "title": "Literature",
    "section": "SceneNet RGB-D: Can 5M Synthetic Images Beat Generic ImageNet Pre-training on Indoor Segmentation?",
    "text": "SceneNet RGB-D: Can 5M Synthetic Images Beat Generic ImageNet Pre-training on Indoor Segmentation?\nSceneNet RGB-D\n\nDataset\nProvides pixel-perfect ground truth for scene understanding problems (semantic segmentation, instance segmentation, object detection)\ncamera poses, depth data (allows optical flow, camera pose estimation, 3d scene labeling)\nrandom scene layouts, physically simulated object configurations.\n\nGoal:\n\nComparison of semantic segmentation performance of SceneNet vs. VGG-16 ImageNet.\nBoth fine tuned on the same SUN RGB-D and NYUv2 Datasets\nWith Depth data included the performance is even better.\nLarge-scale synthetic datasets with task-specific labels &gt; real-world generic pre-training\n\nWhat is interesting for me here?\n\nThere were some open questions in the original SceneNet paper that could be answered here.\nMore realistic rendering with raytracing\nAddition of Physics engine for object placement instead of just the annealing process.\n\nThe problem of Getting good data\n\nA core need in developing automated methods for scene understanding is having good labeled data with as much information as possible.\nImageNet was a first step in this direction.\nObtaining more Data such as RGB-D data is very hard though if done manually.\nOne step in this direction has been done by sceneNN and scanNet, which use reconstructions from path to get the scene geometry and manually annotated the resulting 3d scenes.\nGetting other and more reliable data is even more complicated from the real world, and requires additional equipment or is not even possible, e.g. wen thinking about dynamic scenes.\nGenerating high quality synthetic data with realistic object placement, rendering, human like camera poses and visual effects has the potential to solve many of these problems effectively\n\nContributions:\n\nVery large dataset with high-quality ray-traced RGB-D images, with lighting effects, motion blur, ground truth labels\nDataset Generation pipeline relying on fully automatic randomised methods wherever possible.\nProposition of an algorithm to generate camera trajectories\nComparison of a Pretrained RGB-CNN from synthetic data with one that is trained on real-world data.\n\nWhat is actually interesting here is that they are using this randomized physics based object placement approach and completey left out the object placement approach that we saw in the normal SceneNet",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#blenderproc-scenenet-main.py",
    "href": "core.html#blenderproc-scenenet-main.py",
    "title": "Literature",
    "section": "BlenderProc SceneNet main.py",
    "text": "BlenderProc SceneNet main.py\nParser:\n\nWe read one scene file using the scene_net_obj_path, which also references the associated .mtl material file.\nThe scene_texture_path defines the folder in which all the textures are stored. These are used to map them to the individual objects corresponding to the object types.\nThe output_dir is just the path to where the generated hdf5 files will be saved\n\nLabel Mapping:\n\nI don’t really understand how the label mappings work. We use a nyu_idset.csv file, that is a internal file from the blenderproc utilities.\nMy guess would be theat the load_scenenet method can somehow extract an identifier from the obj or material file, that using this object type mapping can be use to infer object labels in the custom property category_id.\n\nObjects:\n\nwe use the special load_scenenet method that loads the obj file, and maps the textures from the folder to the object using the previously computed label_mapping.\n\nHandle Floors and Walls:\n\nLook for all walls by filtering the loaded objects by the custom property category_id and looking for the id wall.\nFrom these wall objects we extract floors using a builtin BlenderProc method and rename these newly generated objects as floor.\nWe do the same with the ceilings, with the same builtin method but now looking for the inverse up-vector.\nBoth the newly created floors and ceilings get set a custom property of either “floor” and “ceiling”\n\nHandle Lighting\n\nLamps should emit light, so we look for lights by their name using a regex and add a light surface with a relatively high emission, and having the emission color of the material defined for the lamp.\nAlso the ceilings emit a small bit of light for lighting up the whole room, simulating maybe some light coming in from the windows.\n\nFrom the objects we create a bounding volume hierarchy.\nFinding a camera location:\n\nWe try 10000 time to find 5 poses\nWe sample a location above the floor level, at minimum 1.5 meter and maximum 1.8 meter height.\nWe check that we don’t stand on a object with the camera\nWe find some random orientation for the camera\nWe check that there are no objects directly in front of the camera (1.0 meter)\nWe check that we have a good coverage of the scene with the objects that we are looking at. For doing that we use the bounding volume hierarchy of the scene.\nOnce we passed all these checks we add the resulting pose to the valid cam poses and try again until we found 5 of them.\n\nNow that we have objects loaded, floors and ceilings seperated, and defined good poses for the camera we render the normal maps, depth map, segmentation maps using the custom property category_id\nThen we just render the scene and write the results in the hdf5 format to the output dir.",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#blenderproc-scenenetloader.py",
    "href": "core.html#blenderproc-scenenetloader.py",
    "title": "Literature",
    "section": "BlenderProc SceneNetLoader.py",
    "text": "BlenderProc SceneNetLoader.py",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#blenderproc-configuring-the-camera",
    "href": "core.html#blenderproc-configuring-the-camera",
    "title": "Literature",
    "section": "BlenderProc Configuring the Camera",
    "text": "BlenderProc Configuring the Camera\nThe docs talk about how to correctly configure a camera in blenderproc.\nThere are camera intrinsics and extrinsics that you can define. So focal length, optical center vs. position and orientation in the world.\nThe intrinsics are represented by a matrix \\(K = \\begin{pmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1\\end{pmatrix}\\)\nUsing that matrix and the image width and height we can define the full camera intrinsic parameters.\nAlternatively blendproc also can set the camera through blenders own camera parameters, where the focal length or field of view can be defined to set the camera intrinsics.\nNow to get a pose for the camera a transformation matrix can be defined that maps from camera to world coordinate system. This is passed onto the camera as a pose. The camera then when rendering unses these poses and renders an image for each of these.",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#the-coco-format",
    "href": "core.html#the-coco-format",
    "title": "Literature",
    "section": "The COCO format",
    "text": "The COCO format\nJSON format, all annotation share the same basic datastructures.\n{\n\"info\": info,\n\"images\": [image],\n\"annotations\": [annotation],\n\"licenses\": [license],\n}",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#my-takeaways",
    "href": "core.html#my-takeaways",
    "title": "Literature",
    "section": "My takeaways",
    "text": "My takeaways\nLarge scale labelled datasets are important for supervised learning algorithms:\n\nWe dont even want to do supervised learning in any form, but we want labelled datasets of 3d scenes that we can render into rgb-d, semantic segmentation and instance segmentation data.\nInstead of loading arbitrary scenes and inferring object semantics, why not just generate arbitrary scenes that are still realistic.\nOr at least use pre generated scenes for first simple tasks that already contain all the information that we need.\nThis full control over a scene and it’s generation, gives us the power to also apply this to scene abstraction.\nThe concept of there being rules and constrained on how a realistic scene is composed and laid out can be reused when abstracting a scene.\nThe control over the scene generation allows us to replace individual object instances with abstracted object instances and to create a fully abstracted scene from a composition of abstracted objects.\n\nHonestly one bigger question that arises now really is what to do with all that what we know of how BlenderProc works. We have at least some amount of useful tooling now availale. That said here comes then the question of what it really is what we want to achieve at the moment and how to vaidate if we achieved it successfully.\nThis is kind of an open but an important question that my goal is to define today. I just really dont know what i am doing here today. No clue.",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "core.html#key-information",
    "href": "core.html#key-information",
    "title": "Literature",
    "section": "Key Information",
    "text": "Key Information\n\nWhat is abstract?\n\nMathematical Abstraction (bounding with intervals)\nConceptual Abstraction (represent a family of possible renderings rather than one fixed image)\n\n\n\nsource\n\nfoo\n\n foo ()",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "BlenderProc Test Scenes",
    "section": "",
    "text": "Sadly it is not possible to develop a blenderproc pipeline in a notebook fully. Blenderproc needs to execute in a BlenderInternal python interpreter.\nWe can still develop some functionality in notebooks, but only things that can be run without requiring to import the blenderproc libraries.",
    "crumbs": [
      "BlenderProc Test Scenes"
    ]
  },
  {
    "objectID": "test.html#datasets",
    "href": "test.html#datasets",
    "title": "BlenderProc Test Scenes",
    "section": "Datasets",
    "text": "Datasets\nBlenderProc provides easy access to many datasets and freely available assets that can be used for testing using the blenderproc download command.\nOnce you download these assets to your system they can be implemented using different loaders that convert the dataset specific formats into MeshObject instances.\ne.g.: objs = bproc.loader.load_obj(\"mymesh.obj\")",
    "crumbs": [
      "BlenderProc Test Scenes"
    ]
  },
  {
    "objectID": "test.html#manipulating-objects",
    "href": "test.html#manipulating-objects",
    "title": "BlenderProc Test Scenes",
    "section": "Manipulating objects",
    "text": "Manipulating objects\nOnce loaded objects can be manipulated using built in functions. e.g. changing the location, rotation, full pose matrix or transformation matrix.",
    "crumbs": [
      "BlenderProc Test Scenes"
    ]
  },
  {
    "objectID": "test.html#custom-properties",
    "href": "test.html#custom-properties",
    "title": "BlenderProc Test Scenes",
    "section": "Custom properties",
    "text": "Custom properties\nAssigning custom properties might be interesting for semantic segmentation this can be done with the commands obj.set_cp(\"my_prop\", 42) or obj.get_cp(\"my_prop\")",
    "crumbs": [
      "BlenderProc Test Scenes"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blenderproc-test-scenes",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "blenderproc-test-scenes"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "blenderproc-test-scenes",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall blenderproc_test_scenes in Development mode\n# make sure blenderproc_test_scenes package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to blenderproc_test_scenes\n$ nbdev_prepare",
    "crumbs": [
      "blenderproc-test-scenes"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "blenderproc-test-scenes",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/flupppi/blenderproc-test-scenes.git\nor from conda\n$ conda install -c flupppi blenderproc_test_scenes\nor from pypi\n$ pip install blenderproc_test_scenes\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "blenderproc-test-scenes"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "blenderproc-test-scenes",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "blenderproc-test-scenes"
    ]
  }
]